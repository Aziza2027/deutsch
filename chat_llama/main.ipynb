{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The image shows a group of individuals, some wearing face masks and protective garments that suggest they are in a professional setting related to medical or health services. They are standing together in an indoor environment with what appears to be a laboratory or healthcare facility in the background. In their hands, several of them are holding bowls with food items. One person is holding a bowl with what looks like slices of cheese pizza, and another individual is holding a bowl with what appears to be snack food, possibly chips or crackers. There's also a cake with candles on it, indicating a celebration or special occasion. The image has a warm and cheerful atmosphere as if the group is posing for a photo during a break or event. \n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "res = ollama.chat(\n",
    "\tmodel=\"llava:7b\",\n",
    "\tmessages=[\n",
    "\t\t{\n",
    "\t\t\t'role': 'user',\n",
    "\t\t\t'content': 'what is in the image?', # or any prompt regarding the image\n",
    "\t\t\t'images': ['/Users/aziza/Downloads/1.jpg']\n",
    "\t\t}\n",
    "\t]\n",
    ")\n",
    "print(res['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--Please list the 3 largest cities in the world.--\n",
      "\n",
      "\n",
      " The three largest cities in the world by population as of 2021 are:\n",
      "\n",
      "1. Tokyo, Japan\n",
      "2. Delhi NCR, India\n",
      "3. Shanghai, China "
     ]
    }
   ],
   "source": [
    "\n",
    "# Importing the required library (ollama)\n",
    "import ollama\n",
    "\n",
    "# Initializing an empty list for storing the chat messages and setting up the initial system message\n",
    "chat_messages = []\n",
    "system_message='You are a helpful assistant.'\n",
    "\n",
    "# Defining a function to create new messages with specified roles ('user' or 'assistant')\n",
    "def create_message(message, role):\n",
    "  return {\n",
    "    'role': role,\n",
    "    'content': message,\n",
    "    'images': ['/Users/aziza/Downloads/1.jpg']\n",
    "  }\n",
    "\n",
    "# Starting the main conversation loop\n",
    "def chat():\n",
    "  # Calling the ollama API to get the assistant response\n",
    "  ollama_response = ollama.chat(model=\"llava:7b\", stream=True, messages=chat_messages)\n",
    "\n",
    "  # Preparing the assistant message by concatenating all received chunks from the API\n",
    "  assistant_message = ''\n",
    "  for chunk in ollama_response:\n",
    "    assistant_message += chunk['message']['content']\n",
    "    print(chunk['message']['content'], end='', flush=True)\n",
    "    \n",
    "  # Adding the finalized assistant message to the chat log\n",
    "  chat_messages.append(create_message(assistant_message, 'assistant'))\n",
    "\n",
    "# Function for asking questions - appending user messages to the chat logs before starting the `chat()` function\n",
    "def ask(message):\n",
    "  chat_messages.append(\n",
    "    create_message(message, 'user')\n",
    "  )\n",
    "  print(f'\\n\\n--{message}--\\n\\n')\n",
    "  chat()\n",
    "\n",
    "# Sending two example requests using the defined `ask()` function\n",
    "ask('Please list the 3 largest cities in the world.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--what do you see in the image?--\n",
      "\n",
      "\n",
      " In the image, there are several individuals wearing protective clothing and gloves, which suggests they may be in a medical or healthcare setting. They are holding food items, possibly snacks, that look like chips or popcorn. A few people are standing in front of a building with a cardboard partition, and the environment looks like it could be an educational or research facility given the professional attire and the presence of equipment. There is also a person at the back who appears to be overseeing the gathering or taking part in the activity. The setting includes a white board or wall in the background, which might indicate a lecture hall or a conference room within the building. "
     ]
    }
   ],
   "source": [
    "\n",
    "ask('what do you see in the image?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--is there any person wearing a hat in the image?--\n",
      "\n",
      "\n",
      " Yes, there is a person in the image wearing a hat. They are positioned towards the left side of the group holding food items. "
     ]
    }
   ],
   "source": [
    "ask('is there any person wearing a hat in the image?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--is there any person wearing glasses(yes or no)--\n",
      "\n",
      "\n",
      " No, there are no people wearing glasses in the image. "
     ]
    }
   ],
   "source": [
    "ask('is there any person wearing glasses(yes or no)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--but isnt the girl in the middle wearing glasses?--\n",
      "\n",
      "\n",
      " Yes, the girl in the middle is wearing glasses. "
     ]
    }
   ],
   "source": [
    "ask('but isnt the girl in the middle wearing glasses?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--and isnt the guy on the left  wearing glasses?--\n",
      "\n",
      "\n",
      " Yes, the person on the left is wearing glasses. "
     ]
    }
   ],
   "source": [
    "ask('and isnt the guy on the left  wearing glasses?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--are you sure can you send the cropped image of the guy on the left?--\n",
      "\n",
      "\n",
      " It seems like you're asking for a specific part of an image, which might not be feasible without additional context. However, I can suggest some steps to help you find the cropped image or the individual you are interested in:\n",
      "\n",
      "1. Use online tools like reverse image search (e.g., Google Images, TinEye) to find similar images that might include a cropped version of the person you're looking for.\n",
      "2. If the photo is on a social media platform or an online gallery, check if there are any features that might help you identify the cropped image. For example, metadata such as file name, EXIF data (e.g., date and time), or watermark could be useful in identifying the source of the image.\n",
      "3. If you have the original image, use photo editing software to crop the image as per your requirements. Make sure to save the cropped image with a different file name to avoid overwriting the original.\n",
      "4. Once you have identified the cropped image or the individual, you can provide that information or share the cropped image if it's within the context of the discussion.\n",
      "\n",
      "Please note that finding specific parts of an image and identifying people in them may not always be possible depending on the quality of the image, any blurring, and other factors. "
     ]
    }
   ],
   "source": [
    "ask('are you sure can you send the cropped image of the guy on the left?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--so you cannot do it? isnt it easy to crop the image for you?--\n",
      "\n",
      "\n",
      " I'm sorry, but as an AI, I can't physically manipulate images or perform any actions on them. However, if you have a specific image in mind and need assistance with cropping it or finding similar images, feel free to provide more details, and I can offer guidance on how to proceed. "
     ]
    }
   ],
   "source": [
    "ask('so you cannot do it? isnt it easy to crop the image for you?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'models': [{'name': 'llama3.1:latest',\n",
       "   'model': 'llama3.1:latest',\n",
       "   'modified_at': '2024-10-05T17:03:28.742757339+05:00',\n",
       "   'size': 4661230766,\n",
       "   'digest': '42182419e9508c30c4b1fe55015f06b65f4ca4b9e28a744be55008d21998a093',\n",
       "   'details': {'parent_model': '',\n",
       "    'format': 'gguf',\n",
       "    'family': 'llama',\n",
       "    'families': ['llama'],\n",
       "    'parameter_size': '8.0B',\n",
       "    'quantization_level': 'Q4_0'}}]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.pull(\"llava:7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is very basic example \n",
    "import ollama\n",
    "from ollama import generate\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# processing the images \n",
    "def process_image(image_file):\n",
    "    print(f\"\\nProcessing {image_file}\\n\")\n",
    "    with Image.open(image_file) as img:\n",
    "        with BytesIO() as buffer:\n",
    "            img.save(buffer, format='PNG')\n",
    "            image_bytes = buffer.getvalue()\n",
    "\n",
    "    full_response = ''\n",
    "    # Generate a description of the image\n",
    "    for response in generate(model='llava', \n",
    "                             prompt='describe this image and make sure to include anything notable about it (include text you see in the image):', \n",
    "                             images=[image_bytes], \n",
    "                             stream=True):\n",
    "        # Print the response to the console and add it to the full response\n",
    "        print(response['response'], end='', flush=True)\n",
    "\n",
    "\n",
    "process_image(\"image.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LlavaProcessor' from 'transformers' (/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LlavaProcessor, LlavaForCausalLM\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'LlavaProcessor' from 'transformers' (/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import LlavaProcessor, LlavaForCausalLM\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "PATH_TO_CONVERTED_WEIGHTS = \"shauray/Llava-Llama-2-7B-hf\"\n",
    "\n",
    "model = LlavaForCausalLM.from_pretrained(PATH_TO_CONVERTED_WEIGHTS,\n",
    "device_map=\"cuda\",torch_dtype=torch.float16).to(\"cuda\")\n",
    "processor = LlavaProcessor.from_pretrained(PATH_TO_CONVERTED_WEIGHTS)\n",
    "\n",
    "url = \"https://llava-vl.github.io/static/images/view.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n",
    "prompt = \"How can you best describe this image?\"\n",
    "\n",
    "inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(\"cuda\",\n",
    "torch.float16)\n",
    "# Generate\n",
    "generate_ids = model.generate(**inputs, \n",
    "    do_sample=True,\n",
    "    max_length=1024,\n",
    "    temperature=0.1,\n",
    "    top_p=0.9,\n",
    ")\n",
    "out = processor.decode(generate_ids[0, inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True).strip()\n",
    "\n",
    "print(out)\n",
    "\n",
    "\"\"\"The photograph shows a wooden dock floating on the water, with mountains in the background. It is an idyllic scene that captures both\n",
    "nature and human-made structures at their finest moments of beauty or tranquility depending upon one's perspective as they gaze into it\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LlavaForConditionalGeneration' from 'transformers' (/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoProcessor, LlavaForConditionalGeneration\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m LlavaForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllava-hf/llava-1.5-7b-hf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m processor \u001b[38;5;241m=\u001b[39m AutoProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllava-hf/llava-1.5-7b-hf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'LlavaForConditionalGeneration' from 'transformers' (/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n",
    "processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n",
    "\n",
    "prompt = \"USER: <image>\\nWhat's the content of the image? ASSISTANT:\"\n",
    "url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "\n",
    "# Generate\n",
    "generate_ids = model.generate(**inputs, max_new_tokens=15)\n",
    "processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52c2967d5f1a43ab98b6f3d75afa0085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/950 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'llava'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllava-hf/llava-1.5-7b-hf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage-to-text\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/ai2d-demo.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(requests\u001b[38;5;241m.\u001b[39mget(url, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mraw)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/pipelines/__init__.py:741\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    738\u001b[0m                 adapter_config \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m    739\u001b[0m                 model \u001b[38;5;241m=\u001b[39m adapter_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase_model_name_or_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 741\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_from_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    742\u001b[0m     hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39m_commit_hash\n\u001b[1;32m    744\u001b[0m custom_tasks \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:1041\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[0;32m-> 1041\u001b[0m     config_class \u001b[38;5;241m=\u001b[39m \u001b[43mCONFIG_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class\u001b[38;5;241m.\u001b[39mfrom_dict(config_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwargs)\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1044\u001b[0m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[1;32m   1045\u001b[0m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:734\u001b[0m, in \u001b[0;36m_LazyConfigMapping.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extra_content[key]\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping:\n\u001b[0;32m--> 734\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[1;32m    735\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping[key]\n\u001b[1;32m    736\u001b[0m module_name \u001b[38;5;241m=\u001b[39m model_type_to_module_name(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'llava'"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoProcessor\n",
    "from PIL import Image    \n",
    "import requests\n",
    "\n",
    "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "pipe = pipeline(\"image-to-text\", model=model_id)\n",
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/ai2d-demo.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# Define a chat history and use `apply_chat_template` to get correctly formatted prompt\n",
    "# Each value in \"content\" has to be a list of dicts with types (\"text\", \"image\") \n",
    "conversation = [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "          {\"type\": \"text\", \"text\": \"What does the label 15 represent? (1) lava (2) core (3) tunnel (4) ash cloud\"},\n",
    "          {\"type\": \"image\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "\n",
    "outputs = pipe(image, prompt=prompt, generate_kwargs={\"max_new_tokens\": 200})\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
